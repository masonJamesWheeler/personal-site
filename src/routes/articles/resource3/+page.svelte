<h1>
      Annu. Rev. Polit. Sci. 2020.23:401-419. Downloaded from www.annualreviews.org Access provided by 2601:602:9603:3780:d1dc:e786:8ac6:bc6 on 03/26/22. See copyright for approved use.
 
      402 Roberts
      INTRODUCTION
      While the Internet has long been touted as a technology that is difficult to censor, regimes around the world have adopted a wide variety of censorship technologies and online propaganda strate- gies to try to control it. As a result, a rich debate has emerged as to whether the Internet solidifies or undermines autocratic rule. Some scholars have called attempts to control the Internet futile because the controls can often be easily circumvented (Diamond 2010). Others have a much more dire view of the ability of governments and powerful interests to manipulate the information en- vironment and limit their own accountability (Morozov 2011, MacKinnon 2012).
      Much of the theoretical and empirical literature surrounding this debate centers on autocrats and the incentives, strategies, and tools they have to carry out censorship and propaganda on- line. However, an equally relevant part of the puzzle is how citizens react to government efforts to control information online. Internet activist John Gilmore once posited that the Internet was impervious to censorship because “[t]he Net interprets censorship as damage and routes around it” (Elmer-Dewitt 1993). But to what extent do Internet users actually route around censorship? The degree to which social media users recognize censorship and go out of their way to find infor- mation is central to how information spreads online in spite of information manipulation. Users’ willingness to circumvent censorship depends crucially on their incentives and the circumstances surrounding censorship.
      This article reviews a burgeoning literature that studies how individuals respond to online information manipulation. Drawing on this literature, I discuss the contexts in which users are resilient to censorship and the circumstances in which they are affected by it. I make two basic points about resilience to online censorship and discuss their implications for the strategies of authoritarian governments in manipulating information online. First, awareness of information manipulation is key to resilience to that manipulation. Even though some evidence suggests that under certain circumstances awareness of censorship creates chilling effects (e.g., Huang 2015, 2018; Pan & Siegel 2020), awareness of censorship also allows users to find ways to circumvent it and to update their beliefs accounting for censorship. In some cases, awareness of censorship may incentivize users to seek out information their government has concealed (e.g., Nabi 2014, Pan & Siegel 2020). In other cases, awareness of censorship can facilitate coordination, imposing costs on the government for implementing censorship (Hassanpour 2016, Boxell & Steinert-Threlkeld 2019). For those who are unaware that censorship exists and do not know what information might be censored, compensating for information manipulation is very difficult, particularly in online contexts, where censorship is masked by algorithms and the complexity of user interfaces.
      Second, resilience to censorship depends on demand for the information that has been censored and the ability to pay the costs imposed by censorship (Roberts 2018, Chen & Yang 2019). Varia- tion in users’ willingness and abilities to route around censorship creates substantial heterogeneity in censorship resilience across types of information, contexts, and people. The literature suggests that entertainment is more resilient to censorship than political information (e.g., Zuckerman 2015, Hobbs & Roberts 2018). Resilience to censorship is strengthened by education, access to technology, and a wider and more diverse social network (Roberts 2018, Chen & Yang 2019). This heterogeneity in resilience to censorship could make coordination against the government more difficult because segments of the population are exposed to different information environments (Little 2017).
      These insights from the literature offer some explanations for current trends in digital authori- tarianism. First, the importance of awareness of censorship to resilience explains the shift in regime strategies toward more invisible forms of censorship that reduce citizens’ awareness of manipula- tion. It also explains and forecasts an increasing use of microtargeted censorship and propaganda,
      Annu. Rev. Polit. Sci. 2020.23:401-419. Downloaded from www.annualreviews.org Access provided by 2601:602:9603:3780:d1dc:e786:8ac6:bc6 on 03/26/22. See copyright for approved use.
      
      particularly for regimes with the technological capacity to aggregate and use data collected on citizens. Second, the fact that typically many users are unwilling or unable to evade censorship could explain why autocrats have largely been successful at managing the Internet even when they cannot completely control the online environment (Shadmehr & Bernhardt 2015, Chen & Xu 2017). I conclude with a discussion of how current technological advances may impact resilience to censorship and with a call for future research.
      CENSORSHIP TAXES INFORMATION
      To know how people can be resilient to censorship, we have to understand what censorship is. While governments have adopted a huge range of technologies to control information on the Internet, they all share a common characteristic: Censorship technologies all impose taxes on information by requiring users to incur costs for accessing or spreading information. In this way, they incentivize users to consume and spread information that the government would like them to consume because this information is less costly.
      These costs manifest themselves in different ways. I organize the types of costs imposed by censorship into three categories according to the mechanism through which they affect the user: fear, friction, and flooding (Roberts 2018).1
      Censorship through fear imposes costs on users who share and access information by threat- ening these users with some type of costly punishment. Countries around the world have adopted laws and regulations that prohibit users from writing or sharing particular types of information. Many governments resort to extralegal intimidation as a means of deterring users from spreading information. Iran, Russia, China, and Saudi Arabia are all examples of countries that have explicit laws that govern political speech on the Internet. A 2018 Freedom House report found that in 28 of the 65 countries surveyed, someone had been killed or physically attacked by the government or a third party for content they had posted online (Shahbaz 2018). In 43 countries, an online user was arrested or imprisoned for political or social writing. The estimated cost of these punish- ments and probability that these laws will be enforced enter into a user’s cost-benefit calculation in deciding whether or not to spread or access information.
      However, fear is not the only mechanism through which censorship imposes costs on users. Censorship can impose costs simply by making information more difficult to access, a mechanism I call friction (Roberts 2018). Methods of censorship that rely on friction do not necessarily make information explicitly off limits but instead force users to spend more time or money spreading or finding information. Governments that impose firewalls to block websites, require search engines to reorder search results, slow down access, or shut down the Internet all make spreading particular types of information more difficult. Of the 65 countries Freedom House surveyed in its 2018 report, 21 had blocked access to a social media or messaging platform for some amount of time, and 13 had shut down the Internet or mobile phones with a political motivation (Shahbaz 2018). The cost of friction is often imposed in terms of time and knowhow—friction costs users more time and effort to access particular types of information. While friction can often be circumvented, users may not always be willing to pay these costs.
      Similar to friction, flooding is a method by which governments manipulate the relative costs of information, in this case by introducing massive amounts of information in a coordinated way to the digital space. Governments might flood the online space with messages that convey their beliefs or ideology, often known as propaganda. Governments are also known to coordinate
      1Note that there are a number of other ways authors have classified censorship, for example, offline versus online (Sanovich et al. 2018) and by tool (Deibert et al. 2008, 2010, 2011).
      www.annualreviews.org • Resilience to Online Censorship 403
      Annu. Rev. Polit. Sci. 2020.23:401-419. Downloaded from www.annualreviews.org Access provided by 2601:602:9603:3780:d1dc:e786:8ac6:bc6 on 03/26/22. See copyright for approved use.
       
      404 Roberts
      online information—which could be positive messages, entertainment, or other miscellaneous information—in a way that is designed to divert the conversation away from information that re- flects badly on the government (King et al. 2017a, Munger et al. 2019). Alternatively, they might spread false information online in order to confuse users about what is true and what is false, a practice scholars and pundits have called gaslighting ( Jack 2017). [Tucker et al. (2018) provide a good review of literature that studies the types and content of political disinformation.] These efforts require users to pay more costs, in terms of time and energy, to find information about ongoing events and to separate good information from false or biased information.
      While many pundits predicted that the Internet would be difficult to censor, a wide variety of research has shown that government censorship efforts can have a huge impact on information access and—at times—political belief and action. Weidmann et al. (2016) use cross-national mea- surements of Internet penetration to show that politically excluded groups simply have less access to the Internet overall. Boxell & Steinert-Threlkeld (2019) show large impacts of a 2018 social media tax in Uganda on overall social media use. King et al. (2013, 2014) show that the Chinese government individually removes large numbers of social media posts, often related to collective action events, and Bamman et al. (2012) show that removals of posts from Weibo in China are focused in certain geographies, massively affecting the distribution of information available on- line. Roberts (2018) shows large effects of China’s Great Firewall on traffic coming from China to newly blocked websites and impacts of the timing of censorship on the spread of information. Chen & Yang (2019) show that college students in China who are given access to the unfiltered In- ternet and incentivized to use it become more skeptical about government policies and the health of the Chinese economy.
      Other research has shown that government flooding of messages on the Internet can have large effects on the prevalence of content online. Verkamp & Gupta (2013) use five political events from China, Russia, Mexico, and Syria to show that coordinated spam can overwhelm a political hashtag on Twitter, diluting conversations related to the event. King et al. (2017a) estimate that the Chinese government directs the posting of 448 million social media posts per year on Chinese social media websites, largely made up of “cheerleading” posts that distract from ongoing events. Stukal et al. (2017), having developed a method to identify bots on Russian political Twitter, find that bots accounted for more than 50% of tweets on the majority of days between February 2014 and December 2015. Sanovich et al. (2018) use a similar dataset and find that a large portion of bots on Russian political Twitter promote news articles. This activity is likely to increase the search rankings of certain news websites, affecting what users read.
      These findings, combined with online experiments in democracies that show huge impacts of friction on the Internet, suggest that the costs of access to information can have large effects on consumption of information and belief about politics. Epstein & Robertson (2015) use lab and online experiments to show that small changes in the order of results presented by a search en- gine have a large influence on the voting intentions of participants. While the experimental setting makes it difficult to know the external validity of this experiment, this “search engine manipula- tion effect” (Epstein & Robertson 2015) suggests that government manipulation of search engine algorithms could have large effects on political behavior. King et al. (2017b) show that the coor- dinated coverage in national newspapers has large impacts on the distribution of information that is discussed in social media. Participation experiments in democracies, such as Facebook experi- ments (Bond et al. 2012, Jones et al. 2017), have shown that an online nudge to go out and vote can significantly increase the likelihood of participation. These studies suggest that actors with power over what information reaches Internet users, and how quickly, can potentially have a large impact on what users see, what they believe, and when they decide to participate.
      Annu. Rev. Polit. Sci. 2020.23:401-419. Downloaded from www.annualreviews.org Access provided by 2601:602:9603:3780:d1dc:e786:8ac6:bc6 on 03/26/22. See copyright for approved use.
      
      These findings are consistent with a substantial literature on traditional media. Government control of the print media can have a large influence on public opinion and participation when it reduces the number of outlets that individuals have access to, raising the costs of accessing information from independent sources (see Larreguy & Marshall 2019 for a review). For example, Enikolopov et al. (2011) show that the decrease in access to independent television had large and positive impacts on votes for the government party in Russia and substantial negative effects on votes for the opposition. Adena et al. (2015) use geographic and time variation in exposure to Nazi propaganda in prewar Germany to show that areas treated with propaganda were more likely to support the Nazi Party. Yanagizawa-Drott (2014) shows that exposure to radio in Rwanda advocating the killing of Tutsis increased participation in the genocide. McMillan & Zoido (2004) reveal that the Peruvian government was willing to pay millions of dollars in bribes to the media to keep information from citizens, an indication that the value of captured media and control over the distribution of information can be quite high in particular contexts.
      PATHS OF INFLUENCE OF CENSORSHIP
      The studies above provide evidence that censorship influences the distribution of information that citizens have access to. Models of political persuasion and coordination suggest that the dis- tribution of information could influence citizens through several different pathways. First, it could affect citizens through agenda setting or salience, bringing issues that reflect well on the govern- ment to the forefront of citizens’ attention and weighting them highly in importance (Converse 1962, McCombs & Shaw 1972). Research in political communication has shown that the preva- lence of an issue reported in the media can affect how important citizens believe this issue to be (see, e.g., Iyengar 1990, Zaller 1992, Iyengar & Kinder 2010).
      Second, it could affect citizens by exposing them disproportionately to biased information, which citizens could take into account to update their beliefs in ways that are more favorable to the government (for a review, see Gerber & Green 1999). Theories and experimental evidence have shown that recipients of information might update their beliefs in a Bayesian way, updating their priors to adjust to new information (Bullock 2009, Hill 2017). These theories suggest that, all other things equal, the more information a person receives that is favorable about the government, the more likely that person is to also favor the government.
      Third, even if citizens’ beliefs are not impacted by censorship, the distribution of information that users access might influence their propensity to coordinate against the government by in- fluencing their evaluations of what others believe or by impacting the distribution of logistical information about protest. This logic is related to theories that citizens know that political protest can only be successful when others also coordinate; therefore, they rely on information about oth- ers’ likely actions to decide their own (Kuran 1989, 1991; Lohmann 1994; Shadmehr & Bernhardt 2011). Little’s (2016) model shows that communication of the logistics of protests may be the most dangerous aspect of communication technology to authoritarian regimes. These pathways may be particularly important in online protest mobilization, as research has shown that protest diffusion online follows different patterns than offline protest diffusion, allowing for quicker incorporation of a broader population (Bennett & Segerberg 2012), new subpopulations, and new causes (Earl 2010).
      However, these pathways can all be undermined if users are able to notice and resist censorship. Internet users are not always passive consumers of information. On the contrary, there is substan- tial evidence that under certain circumstances users can be very savvy consumers of information, noticing and circumventing censorship. Despite the evidence (described above) that censorship can have a large impact on the distribution of information—and potentially on political belief and
      www.annualreviews.org • Resilience to Online Censorship 405
      Annu. Rev. Polit. Sci. 2020.23:401-419. Downloaded from www.annualreviews.org Access provided by 2601:602:9603:3780:d1dc:e786:8ac6:bc6 on 03/26/22. See copyright for approved use.
      
      406 Roberts
      action—there are time periods when users are willing to route around it. For the remainder of this review, I discuss the circumstances under which users are resilient to the pathways of manipulation described in this section.
      For Internet users, resilience to online censorship depends on a few crucial factors. First, are users aware that censorship is happening and aware of the types of information that it is affect- ing? Can they use this knowledge to update their beliefs to account for censorship? Second, do users want unfiltered information enough to be willing to pay the increased costs of access im- posed by censorship? Third, do users have the resources to evade censorship—in terms of the predisposition, education, money, and social networks that can facilitate its evasion? Last, do users believe that others are aware of and resilient to censorship? I describe each of these factors and the literature that informs them below.
      AWARENESS OF CENSORSHIP AND BACKLASH
      Because of the nature of information, censorship can be disguised, making information manipu- lation difficult to notice. While fear-based censorship—meant to intimidate and deter—must be visible in order to be effective, more sophisticated forms of censorship that work through friction and flooding such as blocking of websites, reordering of search results, and covert information campaigns can exert their effects without users’ awareness (Roberts 2018). For this reason, infor- mation manipulation can easily go undetected, and users may not notice government influence on their information environment.
      Can users resist censorship if they become aware of it? The literature has a mixed response to this question. On the one hand, awareness of censorship could make censorship even more effective. Knowledge of censorship might create chilling effects, motivating users to self-censor. Chilling effects have mostly been documented for journalists in traditional media (Stern & Hassid 2012) or in face-to-face experiments (Young 2019). However, there has been some evidence of online chilling effects as well. For example, Huang (2015, 2018) uses surveys and online survey experiments to show that individuals in China exposed to “hard propaganda” (news articles that are obviously propaganda) have a greater evaluation of state strength and reduced willingness to protest. Pan & Siegel (2020) follow Saudi Arabian Twitter users and show that those imprisoned and tortured in Saudi Arabia for their online speech are less likely to continue speaking out against the government online after they are released. Fu et al. (2013) find that some users on Sina Weibo (China’s version of Twitter) decreased their discussions of political topics after the Chinese gov- ernment implemented a real-name registration policy, and Tanash et al. (2017) find a high rate of self-deletion of Turkish Twitter accounts after the 2016 attempted coup, which suggests that chilling effects may be stronger when the political stakes are higher.
      However, awareness of censorship can also have the opposite effect, drawing users toward the hidden information or creating backlash. Similar to the contentious politics literature, which has provided some evidence of a backlash effect with regard to offline repression (Lichbach 1987, Moore 1998, Goldstone & Tilly 2001, Francisco 2005, Davenport 2007), both Huang (2018) and Pan & Siegel (2020) document online backlash effects associated with awareness of the censorship they study, in addition to chilling effects. Huang (2018) shows that even though users have reduced willingness to protest and increased evaluation of state strength after reading hard propaganda, the propaganda also worsens individuals’ opinions of the regime and makes them more likely to report wanting to move abroad. Pan & Siegel (2020) find that even though social media users who were put in prison for their online writing were chilled into reducing their criticism of the government after they were released, their arrests mobilized their followers to increase the amount of antiregime sentiment expressed on Twitter following their arrest. In
      Annu. Rev. Polit. Sci. 2020.23:401-419. Downloaded from www.annualreviews.org Access provided by 2601:602:9603:3780:d1dc:e786:8ac6:bc6 on 03/26/22. See copyright for approved use.
      
      addition, Pan & Siegel (2020) find increases in Google search volume for information about those arrested after their arrests, indicating that users were becoming more aware of the dissidents’ previous writings as a result of censorship.
      This suggests that censorship itself can provide information to citizens that they can use to update their beliefs. Studies of traditional media show that citizens seek out more reliable infor- mation when they know that official sources are biased. Geddes & Zaller (1989) find that citizens in authoritarian Brazil can resist government propaganda when they are sophisticated enough to recognize it. Stockmann (2012) finds that citizens in China view commercial newspapers as more consistent with public opinion than official newspapers, and shows that readers strategically seek out information from commercial papers and turn to government papers when they find them informative. In a follow-up study on the search engine manipulation effect, Epstein et al. (2017) show that awareness of search engine manipulation can significantly reduce its impact on public opinion. These studies suggest that users can take into account information manipulation when they become aware of it.
      Like the finding in Pan & Siegel (2020) that censorship can draw users toward that which was censored, the literature has documented several cases where censorship has caused more (rather than less) interest in the censored material. Several scholars have found that online censorship can mobilize users against the government or inspire more interest in the concealed content be- cause of censorship. This phenomenon is often called the Streisand effect, having been observed after Barbra Streisand filed a lawsuit to remove photos of her mansion from a website owned by the California Coast Records Project. The lawsuit brought attention to the photos, which then attracted hundreds of thousands of viewers to the website ( Jansen & Martin 2015).
      Nabi (2014) uses case studies in Turkey and Pakistan to study how censorship of YouTube can increase the popularity of the content that is the target of censorship. The author finds that YouTube remained one of the top ten most visited websites for both Turkey and Pakistan even after it was blocked. Further, Google searches for and views of the content that was the target of censorship spiked after the content was censored, indicating that many people may have become aware of or interested in the censored content because of censorship. Nabi (2014) provides evi- dence that many users in both Turkey and Pakistan were obtaining software that allowed them to evade the block and were accessing the YouTube content by routing around censorship.
      Boxell & Steinert-Threlkeld (2019) study the implementation of a social media tax in Uganda. Beginning on July 1, 2018, users were taxed 200 shillings per day for using any one of a set of social media applications, including Twitter, WhatsApp, and Facebook. Even though the tax decreased overall use of these applications, it also inspired backlash. Not only were authors on social media writing more about collective action after the tax, but also the incidence of protests in Uganda increased by 48%, including protests against the social media tax itself.
      These studies that document censorship’s backlash effect are consistent with empirical work that shows a positive relationship between connectivity and pro-government views, indicating that more censorship can sometimes be a detriment to authoritarian regimes. Kern & Hainmueller (2009) show that individuals who circumvented government censorship in East Germany to ac- cess West German television had a more positive opinion of the East German regime, likely be- cause it allowed them access to entertainment that improved their daily lives. Hassanpour (2014, 2016) uses Internet blackouts in Egypt and Syria to argue that lapses in connectivity created “small world networks” by encouraging face-to-face interaction. The Egyptian Internet and media black- out during the Tahir Square protests increased rather than decreased coordination, perhaps be- cause people began seeking information in face-to-face encounters and local, more dense networks (Hassanpour 2014). The Internet blackout in Egypt during the 2011 protest movement made apolitical citizens more aware of and interested in the unrest, and protests coordinated in small
      www.annualreviews.org • Resilience to Online Censorship 407
      Annu. Rev. Polit. Sci. 2020.23:401-419. Downloaded from www.annualreviews.org Access provided by 2601:602:9603:3780:d1dc:e786:8ac6:bc6 on 03/26/22. See copyright for approved use.
      
      408 Roberts
      local networks were more difficult for the government to control than the centrally coordinated protests had been. Similarly, Hassanpour (2016) uses a Syrian Internet blackout that originated from abroad in November 2012 to gain insight on how the Syrian conflict was affected by media disruption. He finds that the disruption increased violent incidents on the part of the rebels, in an uncoordinated way, and spread them to new locations, suggesting that there was more dispersed coordination as a result of the outage.
      The literature not only shows backlash effects in response to censorship but also suggests that information manipulation might reduce the government’s ability to refute misinformation, be- cause citizens discount these refutations. Huang (2017) studies the impact of antigovernment political rumors in China through an online survey experiment. He finds that even when these rumors are rebutted, they significantly decrease citizen trust in the government. Furthermore, government rebuttal of these rumors is less effective than a government critic’s rebuttal, indicat- ing that users may discount rebuttals of antigovernment rumors that originate from government sources.
      These documented cases of backlash have one thing in common: They all occurred when cen- sorship was visible and affected many users at the same time. Censorship affected one of the most popular websites in both Turkey and Pakistan (Nabi 2014), where the inability to access YouTube likely alerted many users to censorship who had not been aware of it before. Similarly, the social media tax in Uganda (Boxell & Steinert-Threlkeld 2019) is a very visible form of censorship af- fecting the entire population of Internet users; it serves as a focal point for coordination. Huang (2018) shows that hard propaganda draws awareness toward censored material because of its vis- ibility. More research could be conducted to pinpoint the conditions under which awareness of censorship creates backlash. In the next section, I discuss research that draws on these findings to explain how regimes are adapting their censorship techniques to (a) make censorship less visible and (b) remove its coordinating potential in order to harness the Internet for their own interests.
      AUTHORITARIAN RESPONSES TO CENSORSHIP AWARENESS
      Given that awareness of censorship can create more interest in censored material and can lead to backlash, governments have adapted their censorship strategies by only exerting partial control of the Internet through friction and flooding in an effort to hide their manipulation. Formal models of media control in authoritarian regimes point out that authoritarian regimes have incentives to mix biased and unbiased media in order to keep their citizens guessing about the reliability of the information (Gehlbach et al. 2016). Having too much (or too obvious) censorship means that citizens will not believe much of what they read. Shadmehr & Bernhardt (2015) develop a model that shows that autocrats have incentives to credibly commit to censoring less than they will in equilibrium because citizens who are aware of high levels of censorship will update their beliefs with increasingly negative opinions about the government in the absence of news.
      In a similar vein, Chen & Xu (2017) show that when a government can commit to a policy of censorship in advance, it should choose a moderate amount of censorship and should commit to implementing some reforms, rather than choosing high levels of censorship. If citizens know that the government is constrained to not censor all of the time, then, when they see no other citizens supporting a measure, they are more likely to interpret that absence of information as a true reflection of public opinion rather than a product of censorship.
      These models echo other theoretical work that shows that autocrats should have incentives to only partially control information. Because authoritarian governments rely on vertical flows of information to inform them of the state of the world, too much control of the information environment can be detrimental to autocrats’ survival; it impedes collecting information from
      Annu. Rev. Polit. Sci. 2020.23:401-419. Downloaded from www.annualreviews.org Access provided by 2601:602:9603:3780:d1dc:e786:8ac6:bc6 on 03/26/22. See copyright for approved use.
      
      and responding to citizens (Egorov et al. 2009, Lorentzen 2014, Qin et al. 2017). In addition, information manipulation is economically expensive and difficult to implement (Miller 2018), and governments may face a trade-off between investing in the economy and suppressing information (Guriev & Treisman 2019).2
      Even though many pundits predicted that the Internet would be a death knell to authoritari- anism, the implication of this literature suggests that the difficulty governments have in exerting complete control over the Internet might actually provide benefits to regimes by imparting the perception of lower censorship and freer information (Shadmehr & Bernhardt 2015). Because the authoritarian regime’s partial control of the online space will occasionally bring negative news to the attention of citizens, the absence of bad news can be interpreted as a positive signal for the regime even when that absence was brought about by censorship controls. Partial control could also decrease the likelihood of backlash against censorship by reducing awareness of censorship, whereas complete control might draw attention to censorship.
      Regimes with the capacity to implement subtle forms of censorship, for example by controlling the results of search engines (without bringing attention to this control) or selectively removing social media posts, may reap the benefits of online censorship without incurring the risks of aware- ness. Fear-based censorship relies on the awareness of users—users cannot be deterred if they are not aware of what information is off limits and of the punishment that might befall them for spreading that information. Censorship that relies on the mechanisms of friction and flooding, in contrast, does not have to be visible to users. This may be why we see a move by many regimes toward less obvious forms of censorship (Roberts 2018, Guriev & Treisman 2019).
      Because the technology behind the provision of information on the Internet is complicated, manipulation can be made invisible. The ambiguity between whether a slowdown or outage results from a technical failure or some political manipulation can give the government the cover of plausible deniability. Even for experts, censorship can be hard to distinguish from a technical error. A recent Citizen Lab report details the extensive methods that experts used to determine whether the block of the Internet Archive from Jordan was a network error or a Jordanian government block (al Masri 2017). Significant efforts must be made to identify filtered images on China’s WeChat or to identify when a topic is subject to search filtering on Baidu (Knockel et al. 2017). The average person’s level of knowledge about the Internet is quite low. Survey data indicate that 48% of Internet users in China are unaware that the Great Firewall exists (Roberts 2018). In developing countries, some online surveys have suggested that many users are unaware that Facebook is a part of the Internet (Mirani 2015). This type of confusion about the functioning of the Internet could make it more difficult to detect government interference and subsequently route around it. It might also reduce the ability for users to view censorship as a focal point around which to coordinate.
      Regimes that do not have the technological capacity to implement subtle forms of censorship and regimes whose citizens are primarily users of Western social media platforms may have a more difficult time asserting control over the digital sphere because any effort at censorship will be easily noticed by users. In Edmond’s (2013) theoretical model, the degree to which autocrats have economies of scale in controlling the media is essential to whether citizens can coordinate against that control. Pan (2016) shows that social media websites used in the vast majority of countries are owned by US firms, which makes subtle forms of censorship more difficult because these governments have less control over the content on the social media platforms. While countries can request that content be removed from US social media websites, this removal can be denied and
      2See Petrova (2011) and Gehlbach & Sonin (2014) for a discussion of economic incentives and media bias in traditional media.
      www.annualreviews.org • Resilience to Online Censorship 409
      Annu. Rev. Polit. Sci. 2020.23:401-419. Downloaded from www.annualreviews.org Access provided by 2601:602:9603:3780:d1dc:e786:8ac6:bc6 on 03/26/22. See copyright for approved use.
       
      410 Roberts
      is often slowly implemented, and US companies often publicize this censorship. China and Iran, where domestic firms control more than half of the social media websites, are notable exceptions to this pattern (Pan 2016). For countries without control over domestic social media websites, Internet blackouts or sudden filtering of Western social media platforms—i.e., visible forms of censorship—may be the only option.
      The fact that awareness of censorship can facilitate resilience to information manipulation also may be the reason that governments and powerful interests are using flooding strategies to manip- ulate information online. A Freedom on the Net 2018 report estimates that, in around half of the 65 countries surveyed, progovernment commentators manipulated online discussions (Shahbaz 2018). Because of the anonymity of the Internet, these efforts may go unnoticed by unsuspecting citizens. In addition, they are relatively cheap for any government to use and therefore do not require control over domestic social media.
      Data collected on the Internet may also allow authoritarian regimes to engage in more targeted repression, in order to intimidate influential users without calling attention to censorship more broadly. Gohdes (2020) discusses the trade-off that regimes face between surveillance—with an open and unaffected Internet—and censorship. Drawing on data from Syria, she finds that the regime is able to use more targeted repression when levels of Internet accessibility are high and must resort to more indiscriminate campaigns of violence when users have more limited access to the Internet. This suggests that the online environment can enable and encourage governments to engage in targeting.
      Not only may the Internet allow regimes to target fear-based censorship, online microtarget- ing might allow them to direct friction and flooding to individual users. As vast amounts of data on users become available through social media companies, companies have the ability to personalize information to users and conduct experiments to understand how information affects user be- havior. Through the advertising platforms of these companies or by investing in these companies directly, influential interests and governments increasingly have the ability to target posts toward particular types of users. Evidence suggests that the Internet Research Agency at the behest of the Russian government used Facebook’s microtargeting capacity to personalize political messages to US Internet users (Lapowsky 2018). Roberts et al. (2020) show evidence that content removal in China might be personalized toward individual users. If users experience information manipula- tion differently, detection of this manipulation will be more difficult, and coordinated backlash may be less likely. A Pew research survey shows that most social media users in the United States do not understand the logic behind the way information is presented to them on social media and are unaware that their personal data are used in advertising (Gramlich 2019). More research is needed on how governments use large amounts of data on populations to target and personalize censorship and what effects this might have on resilience; Tufekci (2014) provides an overview of some of the important issues at stake.
      HETEROGENEITY IN CENSORSHIP RESILIENCE
      While awareness of censorship may be close to a necessary condition for resilience to online cen- sorship, it is not sufficient. The backlash to censorship described in the previous sections depends on whether users who are aware of censorship are motivated and able to circumvent it and resist it. In this section, I highlight literature that has documented how demand for the censored infor- mation is necessary for resilience to censorship, and that the costs of resisting censorship also are important to resilience. I discuss how variation in the ability and willingness to circumvent cen- sorship creates substantial heterogeneity in resilience to it. I explore what implications this might have for citizens’ ability to use the Internet to coordinate against the regime.
      Annu. Rev. Polit. Sci. 2020.23:401-419. Downloaded from www.annualreviews.org Access provided by 2601:602:9603:3780:d1dc:e786:8ac6:bc6 on 03/26/22. See copyright for approved use.
      
      It is well established in the political science literature that demand for political information is typically quite low. Downs (1957) calls citizens’ general lack of interest in politics “rational ig- norance,” meaning that for the most part, people rationally should be ignorant of political issues because they are unlikely to be pivotal in those issues. Surveys have documented a very low level of political knowledge among average citizens in democracies (Converse 1964, Popkin 1994). Ra- tional ignorance in politics may be even more likely in authoritarian contexts, where citizens have less control of the political environment than in democracies.
      Even for politically interested citizens who are aware of censorship, the inability to know what is missing might make demand for circumvention low. In a field experiment in China, Chen & Yang (2019) demonstrate the importance of demand for information in censorship evasion. The authors randomly assign college students to a treatment condition where they are given free access to circumvention software for 18 months and a control condition where users are not provided with evasion software. Interestingly, even when provided with free access, treated students did not choose to use the evasion software to bypass censorship and read blocked material. However, after being given monetary incentives to read blocked news, the students did use the software to bypass censorship. More interestingly, after the experiment ended, many of the students continued to use the software and even pay for it in order to continue reading the blocked news. This suggests that while the students did not initially realize that there was censored information they were interested in accessing, once they knew what information could be accessed by routing around censorship they did want to circumvent it.
      Given low demand for political information, resilience to censorship may be stronger when censorship is applied not just to political information but also to entertainment. Zuckerman’s (2015) “cute cat theory of censorship” posits that while demand for political information is of- ten low, Internet platforms that combine entertainment (like photos of cute cats) and politics may be more immune to censorship. This theory would predict general websites (such as YouTube, Facebook, and WhatsApp) that contain both entertainment and political information to be more resilient to censorship than specific websites that mostly offer political information. This argument is consistent with scholarship that shows that consumption of political information increases when it is paired with entertainment. For example, Baum (2002) shows that Americans are more likely to consume news about international politics when the news is paired with human interest stories. Pan & Roberts (2020) show that before the block of Wikipedia, mainland Chinese users largely sought out entertainment content on Wikipedia, but they ended up consuming political content because they were directed to it through the Wikipedia homepage.
      Hobbs & Roberts (2018) describe a “gateway effect,” where evasion of censorship motivated by demand for entertainment leads individuals to access long-blocked political information. They study the case of the block of the social media website Instagram from China during the 2014 Hong Kong protests. They show that the Instagram block (which occurred on the third day of the protests) incentivized users in China to download evasion software in order to continue to access the website. Hobbs & Roberts (2018) show that those who continue to use Instagram are largely apolitical and seem to have been drawn to circumvent censorship in order to use Instagram for entertainment. Still, once across the wall, users began engaging with long-blocked political information on Twitter and Wikipedia.
      While the literature reviewed so far suggests that users may only evade censorship for enter- tainment, political events may also be able to garner the demand necessary to incentivize resilience. Ball-Rokeach & DeFleur (1976) argue that citizens’ attention and dependence on the media are heightened during time periods of social conflict. Using survey data from Central and Eastern Europe, Loveless (2008) finds that seeking information from television, radio, and newspapers is more predominant in countries in the middle of transitions to democracy, and survey respondents
      www.annualreviews.org • Resilience to Online Censorship 411
      Annu. Rev. Polit. Sci. 2020.23:401-419. Downloaded from www.annualreviews.org Access provided by 2601:602:9603:3780:d1dc:e786:8ac6:bc6 on 03/26/22. See copyright for approved use.
      
      412 Roberts
      report less information seeking in democracies that are more consolidated. If information seeking increases during crises, we might expect that resilience to censorship would be stronger during crisis events. Weidmann & Rød (2019) provide evidence that the Internet solidifies authoritarian regimes during normal times but undermines them conditional on a protest event having started, which could indicate that governments have less control during times of crisis. Roberts (2018) finds that survey respondents interviewed after an explosion in Tianjin, China, were much more likely to report having evaded the Great Firewall than respondents interviewed in Tianjin before the explosion, suggesting that the crisis may have prompted users to seek out unfiltered information.
      Demand for information varies not only across time periods but also, enormously, across Internet users. Roberts (2018) uses survey data to show that those who jump the Firewall in China tend to be better educated, have higher incomes, and have more interest in international politics than the average Chinese Internet user. This is consistent with Chen & Yan
</h1>