<h1>
    okay we're gonna talk quantitative
    analysis for some of you this might be a
    refresher for some of you
    um a lot of this will be new
    if it is new to you hit pause review the
    material
    um
    rewind watch different videos read the
    book you have a lot of resources at hand
    I'm going to try to keep this as simple
    as possible
    here is all we're going to cover it's a
    lot I'm sorry
    um so hit pause if you want to start
    your outline for your notes
    I want to start and talk about data data
    are really messy and I want to use this
    example of the introduction survey that
    you took at the beginning of this class
    so in that questionnaire you were asked
    to give your height in centimeters
    and this is my recommendation to you
    whenever you're collecting data you want
    to set up your questionnaire to get the
    data that you want and so in this case
    it was an open-ended question and so
    once you can see on this graphic that
    some people wrote
    CM some people didn't write anything
    some people wrote I am such and such
    centimeters some people also wrote their
    um height in feet and inches and so this
    takes a lot of time to clean up
    thankfully I was able to use chat gbt
    and it cleaned up the data for me so
    that was nice but
    um this is something you need to think
    about that you want to make it so easy
    for people to enter the data that you
    want to work with
    another example was in the survey you
    were asked how many minutes per week do
    you spend listening to music and
    in this case being precise would have
    really helped so don't have high
    expectations for people even though it
    was clear in this question how many
    minutes per week you can see that a lot
    of people answered in hours
    um also some people said it gave ranges
    so
    don't have our expectations for people
    even though I said minutes people didn't
    read it
    um
    and when you think about it on my end
    cleaning the data it ends up taking time
    right if someone says about an hour per
    day I have to take the time to try to
    figure that out I use chat GPT to get
    some of that but I had to do a lot of it
    myself so you want to meet people where
    they're at and when you have a
    questionnaire you want to Pilot test it
    so that someone can say to you you know
    this doesn't make sense to me so in this
    case for example maybe people think more
    of their music listening time in terms
    of hours than minutes or maybe it's just
    hard for people to recall this
    information at all and a good example is
    TV shows
    in general TV shows with commercials are
    either a half hour or one hour with
    commercials that means they're 22 or 44
    minutes so when you say to someone how
    much TV did you watch last week they're
    generally able to come up with that
    answer because those blocks of time are
    in half hours or hours music is a lot
    harder
    question options matter too by giving
    people choices rather than letting
    people type things in it makes it much
    easier to analyze later so in this
    example the question asks what's your
    favorite color and gave a list of a
    bunch of colors however the downside of
    giving options like this is that maybe
    someone's favorite color was missed the
    list wasn't comprehensive maybe the list
    was too long and people were overwhelmed
    by the time they got to the bottom of
    the list also some things like is there
    a real difference between blue and navy
    lavender and purple
    and the other thing that's challenging
    with this you're still going to have to
    manually assign numbers in order to
    calculate mode later
    this in this question option about how
    much do students support husky Athletics
    this is not a true interval measure
    because the option choices are not
    equidistant from each other
    um but it did allow to have a sort of
    pseudo interval measure
    
    we're going to use these questions as a
    basis for understanding so I also wanted
    just to refresh your mind about what
    they were
    so what do we mean when we say
    statistical analysis this is a method
    that's used to identify numerical
    patterns and calculate relationships
    between and among variables
    it applies mathematical principles and
    procedures to analyze interpret and
    interpret what the data suggests about
    the phenomena being studied
    within statistical analysis we have a
    goal of trying to be parsimonious
    parsimony is a goal in quantitative
    research that means that statistical
    analyzes should provide the simplest
    possible approach
    that offers the best explanation of the
    phenomenon being explored
    the actual definition of statistics is
    numerical summaries of data obtained by
    measurement and computation and there's
    three kinds of Statistics that we're
    going to talk about here
    the first is univariate statistics and
    this is just looking at the values of
    one variable at a time like for example
    the average age of participants
    or the average number of minutes
    listening to music
    this is also called descriptives and
    we'll talk more about this in a minute
    bivariate statistics examine
    relationship between two variables so
    for example if age is related to the
    opinion about the character that appears
    as options when playing video games
    multivariate statistics are three or
    more variables
    so if People's use video games in
    addition to their age might be linked to
    their view for desire for more diverse
    set of playable options so univariate
    analyzes are used to describe whereas
    bivariate and multivariate analyzes are
    used to explain
    so let's talk about descriptives first
    univarity analysis AKA descriptives this
    is just an efficient way to describe
    quantitative data these organize and
    summarize the data so that we can
    understand it better
    it's condensed to the some numerical
    indicator that best summarizes the data
    set it might be the typical point in a
    data set or the best representation
    the three most common forms of
    descriptives that we'll see are first
    frequencies which are the percentages of
    each category measures of central
    tendency which we'll talk about in a
    minute and then measures of dispersion
    aggregation is another way to understand
    descriptives usually in quantitative
    research data are drawn individually so
    each person that participates in a
    survey or an experiment we have data
    about them as an individual or in a
    Content analysis each little piece of
    communication content
    and then when we do statistical analyzes
    we're not actually that interested in
    individual answers we're interested in
    the aggregate aggregation is the
    combined forms
    of individual data points about whom or
    which conclusions are drawn so for
    example let's say there was an
    experiment where there was subgroups
    defined by gender identity of the
    participants
    and if men and women respond differently
    then the difference is based on the
    aggregation of the sample by gender if
    we don't Aggregate and do these
    summaries of data points and
    interpretation this data that we have
    like a bunch of rows just doesn't make
    any sense
    however by focusing on Aggregates there
    is a downside that we could fail to
    notice that if there are exceptional or
    extreme cases but in general with most
    statistical analyzes we're thinking
    about aggregation
    so frequencies this is what you're
    asking about what's going on this
    provokes the why question
    frequencies are fundamental to
    understanding you need to run the
    frequencies for every variable that you
    have it shows the number of cases in
    each variable category or level
    um it's everything that is not anonymous
    value
    often displayed as percentages which
    percentages of course are numbers of
    cases divided by the total and this
    allows us to compare against categories
    so here's an example of some frequencies
    in that survey that you did at the
    beginning of the quarter you were asked
    to list what your favorite TV show was
    when you were six years old
    this is an analysis of 83 students 59
    shows were mentioned and this is really
    interesting I think 23 of you said that
    SpongeBob SquarePants was your favorite
    show and you can see that that was by
    far the most popular show
    followed by Hannah Montana the Pokemon
    um Tom and Jerry
    Dora the Explorer and then it goes down
    from there
    so this is a frequency table that lets
    us know how often these things occurred
    so that we can have a sense of what
    happened with the data
    measures of central tendency
    um I know it's confusing because they
    all start with m whatever trick you need
    from school use it these are the
    expressions of the most common values
    within a set of values of a variable
    the mean which is also known as the
    average is dividing the sum of observed
    values by the number of cases so for
    example if I have a bunch of test scores
    let's say I have 25 students I add up
    all their scores and I divide it by 25
    and that gives me the mean or the
    average
    the mean as a measure is sensitive to
    outliers and extreme cases we're going
    to look at some examples of this in a
    minute
    median is the midpoint of a distribution
    so you line up all the scores from
    lowest to highest
    and the median is the midpoint basically
    you divide it in half
    it is not sensitive to outliers we'll
    look at this again in a moment
    mode is the measure that occurs or the
    value that occurs most frequently in the
    data set
    so with those levels of measurement that
    you had
    if you have nominal data you need to
    present the mode if you have ordinal
    data present the median interval ratio
    data generally you present the mean but
    there are exceptions and we're going to
    talk about those in a minute
    we also need to talk a little bit about
    skew we learned about the normal
    distribution
    a while back
    um you can see here that this one in the
    middle has the normal distribution
    normal distribution
    I mean meat and mode are all in the
    middle
    if there is a positive skew to the data
    so go up and over you can see that the
    mean is going to be higher because the
    mean is sensitive to outliers if there's
    a negative skew to the data you can see
    that the mean is on the other side
    um
    when we have outliers it skews the mean
    and so the median is our preferred
    measure when we have outliers we'll look
    at an example of that in a minute
    so from our class data set 84 students
    reported their height in centimeters I I
    do want to say that I think that someone
    might have reported their height in
    inches because there was one value that
    was quite low here I don't know but it
    might have altered the data in this case
    our mean height in centimeters was 171.0
    
    the median was 172.36 and the mode the
    most frequent was 165 and you can see
    that these sort of all cluster around
    the mean
    and I should also mention that height in
    centimeters is a ratio because there is
    no true zero
    so when do we use mean versus median
    this is a really important question
    as I said
    the mean is sensitive to outliers in
    extreme cases because it's the sum
    divided by the number of values whereas
    median is not sensitive to outliers
    so let's imagine that there's a room of
    20 Ordinary People and you walk into
    this party or this room and you have to
    guess their mean and median income
    in general you know borrowing something
    strange going on we'd probably guess the
    mean and median to be pretty similar
    because we're assuming there's no
    extreme outliers in the room so let's
    say that we guess the mean income is 50
    000 per year and the median is 45 000
    per year
    and then a billionaire walks into the
    room this person makes one million
    dollars a year
    what will happen to the mean income
    remember the mean is sensitive
    to outliers and again remember mean
    would be adding up everything
    dividing it by the number of people
    so the Billionaire's income will heavily
    skew the mean and will increase the mean
    significantly so you take that 50
    000
    let's say times 20 people add a billion
    dollars divide it by 21 people now and
    now you have a mean income of 50 million
    dollars instead of a mean income of 50
    000 right so you can see that this
    really screwed up the mean and it's hard
    to know what's going on in the room
    because that one billionaire really
    messed it up
    so this is not a good representation of
    income but the median on the other hand
    is not going to be affected by the
    Billionaire's income because it's still
    going to be the middle value so you can
    see when you're talking about income in
    this way the looking at median is a lot
    more helpful than looking at mean and
    you'll notice if you hear on the news
    that they talk about incomes in Seattle
    or incomes in the United States or
    something they will report using the
    median
    I also want to talk about histograms
    these are useful ways of looking at data
    when we talked about sampling we use an
    example of a histogram from penguin
    flipper lengths
    if you remember
    um and this is just a way of looking at
    if the variable is skewed or not or in
    what direction by visualizing the
    distribution this is our height in
    centimeters
    the x-axis will display the values of
    the variable cells and the y-axis
    displays the corresponding frequencies
    histograms use bars to represent values
    so you can see like here this is like
    151 to 153 in here with different
    heights of the bars indicating the
    frequency that the values appear in that
    block of data
    okay so variability
    this is
    remember we talked about frequencies we
    talked about measures of central
    tendency variability is the degree to
    which values are spread out or clustered
    together
    the range is just subtracting the
    smallest value from the largest and
    usually we report range fully so for
    example let's say you did a study of
    people in the age range was 18 to 65.
    usually people report that whole range
    standard deviation is a very important
    part of variability standard deviation
    tells you how much the individual data
    values deviate from the mean it is the
    average spread of values around the mean
    a big standard deviation tells you that
    something might be up with your data
    things are not very consistent and
    values are more spread out a small
    standard deviation indicates that the
    values are similar
    so as an example imagine that five
    professors are all grading the same
    essays on a one to five scale
    and so for essay one the professors gave
    a four a four a four a five and a three
    for essay 2 they gave a two a three a
    five a four and a one so very different
    scores the standard deviation
    calculation shows that for essay 1 the
    standard deviation 0.63 not too bad
    but for sa2 it's 1.41 it's higher than
    one it lets us know that the jumps
    between these
    away from the mean are really far apart
    so in this case
    it's a red flag that something's
    inconsistent in the grading between
    these professors
    so range and standard deviation are
    super important if you're reporting mean
    you gotta report standard deviation if
    you just tell me the mean scores on the
    test in your class that doesn't mean
    much to me I gotta also know standard
    deviation to really understand what's
    going on
    range and standard deviation are also
    really helpful for noticing errors in a
    data set so for example let's say you do
    a study for ages and everyone that you
    talk to was age 1865. but you go into
    the statistics program and you see the
    range is 9 to 99. someone must have
    entered their age wrong or if the
    standard deviation is huge something's
    wrong in your data so this is useful
    this way too
    looking at the class height in
    centimeters again we already talked
    about the mean median and mode the range
    of heights was 65 to 200.66 so big range
    and the standard deviation was 15.97 and
    really that's those outliers that are
    changing things up I think that it might
    have been a little different if
    we didn't have those outliers but
    you can see that
    um the standard deviation indicates that
    there are some big differences in this
    data set
    here's another example that might make
    it a little clearer this was on a scale
    of one to seven students reporting how
    much they support UW husky Athletics
    um and so these are frequencies of what
    people reported in this case the mean
    was
    um 4.56 the median was 5 the mode is 5
    the range is one to seven and the
    standard deviation was 1.69
    so you can see that in this case that
    you know
    the most people said five
    and the mean is just a little under five
    it's 4.56 it's like right here right but
    um
    the spread is still pretty good with
    1.69
    we'll look at it again here
    um pulled out like this and here's the
    mean and here's the median you can see
    and the mode is five so it's on top
    there too and you can see here this is a
    negative skew
    and the mean is right out in front
    so you can see the majority of the class
    does support husky Athletics pretty
    strongly
    um so this is just another way to
    visualize this
    we're gonna move on to talk about
    inferential explanatory statistics so if
    you want to take a little break right
    now this would be a good time to take a
    break hit the pause button if you had a
    hard time with some of this stuff you
    might want to rewind back and start from
    here
    um and just watch these again
    but let's move on to talking about some
    more analysis
    so recall we talked about bivariate
    multivariate and univariate Analysis
    univariate was when we're just looking
    at single indicators bivariate is
    relationship between two variables
    multivariate is relationship between
    three or more
    in general when we're doing
    explanatory analyzes bivariated
    multivariate analyzes we're trying to do
    one of two things we're either trying to
    test the strength of relationships this
    is also called associations or we're
    trying to test differences between means
    so we're going to talk about how those
    are different
    we've talked about hypotheses in this
    class quite a bit before but I'm going
    to explain
    um about
    statistical significance in hypotheses
    a research hypothesis is a statement
    about the relationships among the
    variables that the researcher intends to
    study
    and then there's this thing called the
    null hypothesis and this can be
    incredibly confusing
    hear me out try to follow along there's
    also videos on this the null hypothesis
    is the opposite of the research
    hypothesis it is a statement
    that is a prediction about what the
    researcher expects not to find the null
    hypothesis is that there's no
    relationship between the variables
    or that there's complete randomness
    okay
    researchers use tests of student
    statistical significance to test their
    hypotheses and these tests calculate the
    likelihood that the observed
    relationships between variables were
    random or just by chance that you know
    did this just happen just because
    so the null hypothesis
    is saying that there's no relationship
    okay
    the research hypothesis says there is a
    relationship
    so we have these types of error that are
    part of this
    type 1 and type 2 errors are mistakes
    that are made when interpreting
    statistical results
    type 1 errors are when someone falsely
    concludes that there is a relationship
    when there actually is not one
    type two errors are when you don't
    detect
    a real effect or relationship but there
    was one there
    so a type 1 error is also known as a
    false positive
    this is what a case when the researcher
    rejects the null hypothesis but the null
    hypothesis was actually true remember
    the null hypothesis was that there's no
    relationship in this case with a type 1
    error the researcher concluded that
    there was a relationship when there
    really wasn't
    so imagine a study where someone is
    studying trying to understand if
    listening to classical music helps
    academic performance
    the null hypothesis in this study is
    that there is no significant difference
    in academic performance between students
    who listen to classical music and
    students who don't listen to classical
    music
    if the researcher finds a statistically
    significant difference between those
    who'd listen to music and didn't but the
    effect is actually just due to chance
    they have made a type 1 error
    in the social sciences we use a five
    percent level for our threshold about if
    
    something is just due to chance
    okay
    so the researcher will go through a test
    
    of statistical significance and ask
    themselves
    is there less than a five percent chance
    that this relationship is there
    a type 2 error is when the researcher
    fails to reject a null hypothesis that
    is actually false this is a false
    negative
    the researcher concludes that there's no
    significant relationship or effect
    between the variables when there
    actually was
    so in this case let's say the researcher
    failed to find a relationship between
    academic performance and classical music
    but there is a real effect they may have
    made a type 2 error by concluding that
    listening to classical music didn't
    improve academic performance in order to
    avoid type 2 error you have to have a
    good sample size that you can actually
    see the relationship
    so why am I telling you all this this
    might seem kind of obscure
    researchers use these tests of
    statistical significance to test their
    hypotheses these tests calculate the
    likelihood that the observed
    relationships between variables were
    just random
    or if there is a real relationship
    between variables
    p-values are measures of statistical
    significance we calculate them with
    statistic software in your textbook they
    talk about SPSS but you can also use
    Excel Google Sheets you can also
    calculate it by hand
    p-values represent the probability of
    obtaining results
    that are as Extreme as what was observed
    or if there's no real relationship
    between the variables as I mentioned a
    moment ago there's a long-standing
    tradition in the social sciences of
    using a
    0.05 significance level which means that
    there's less than five percent chance
    that the results were due to random
    chance
    we usually report results as less than
    .05 less than .01 or less than .001
    
    the lower the p-value the more certain
    do we have that it wasn't just due to
    chance
    so with p
    being less than .05 it means that the
    likelihood of finding the relationship
    between variables that were observed in
    the study
    where there is no such relationship in
    the population is less than five and one
    hundred
    so if a researcher obtains a p-value
    within this range less than .05
    they can reject the null hypothesis so
    the goal is to reject that null
    hypothesis that again that was the
    hypothesis that said that there's no
    relationship between variables the
    smaller the p-value the more plausible
    the research hypothesis is compared to
    the null hypothesis
    if this was a lot for you hit pause
    listen to it again there's also a video
    on this that explains it
    in a different way
    um it is hard I know I know
    
    so how do we get to statistical
    significance you need to have a good
    sample size you have to have a strength
    of relationship between variables and
    with larger sample sizes or stronger
    strength of relationships you have a
    higher likelihood of finding statistical
    significant relationships between
    variables
    I do want to warn you though that
    statistical significance does not
    necessarily imply practical or clinical
    significance practical significance is
    means like that it doesn't actually
    really matter so imagine that a study
    shows a statistically significant
    reduction in depression after a
    treatment but the effect is really tiny
    and the treatment costs millions of
    dollars and it's really hard for people
    to go through this treatment so even
    though there was a statistical
    significance this study might not have
    practical significance because it might
    actually not be worth doing
    clinical significance let's give an
    example of there's a new medication that
    lowers blood pressure by a few points
    but the medication has a bunch of side
    effects even though the study might have
    shown that it's statistically
    significant it's not actually clinically
    significant because doctors aren't going
    to use it because of all these side
    effects
    I also want to mention that inferential
    statistics technically should not be
    used in a non-probability sample because
    inferential statistics really require
    you to have a random sample that goes
    along with the normal distribution
    non-probability samples are less likely
    to
    um be random obviously and they're less
    likely to have a normal distribution all
    of these analyzes assume a normal
    distribution so
    in reality we still do this but you have
    to be much more cautious in the
    assertions you're making when you are
    
    trying to use Implement inferential
    statistics on a non-probability sample
    
    so a good hypothesis has to be
    falsifiable that means it's testable
    it's falsifiable if it can be
    potentially proven false through
    observation or experimentation so for
    example all swans are white is a
    falsifiable statement because it can be
    tested through observation there is a
    God who created the universe is not a
    falsifiable statement because you can't
    test or observe that
    good hypotheses need to also be
    significantly specific concrete and
    clear in order to be tested
    research questions are okay but in
    general we prefer hypotheses research
    questions are only used when there isn't
    enough
    prior knowledge in your literature
    review or if prior research findings
    contradict each other but if you're
    going to do a study that's quantitative
    you really want to try to have a
    hypothesis
    and on top of that you want to try to
    have a directional hypothesis
    non-directional hypotheses they're also
    called two-tailed hypotheses are when
    the researcher predicts that there will
    be a difference between two groups or
    relationship between two variables but
    does not specify what direction the
    difference or relationship goes
    you want to try to have a directional
    hypothesis since it's more precise so a
    directional hypothesis would be as more
    television viewing increases fear of
    crime increases
    a non-directional hypothesis would just
    be the amount of nukes consumption will
    be related to fear of crime or com
    majors and poli-sci Majors will have
    different
    LSAT exam scores
    but you don't predict any particular
    direction that they go in
    try not to do that
    okay
    again bivariate and multivariate
    analyzes are used to explain
    we look at associations and differences
    important to know is that the type of
    statistical analysis that you can do
    depends on two things one the hypothesis
    you have is your hypothesis implying
    Association or is it about comparing
    differences between groups or
    interventions and the level of
    measurement of your variables only some
    tests can be done with some levels of
    measurement so we're going to go through
    them and it's going to be really
    important that you learn
    what levels of measurement go with which
    test
    so first we're going to talk about
    associations and then we're going to
    talk about differences
    the primary Association that we
    understand is called correlation people
    say this casually all the time in
    everyday speech but it does actually
    have a statistical meaning
    the change in one value of one variable
    is associated with the change in the
    other variable
    and a correlation requires two
    continuous variables
    in a correlation you compute something
    called a Pearson's R this Pearson's R
    tells you the direction positive or
    negative and the magnitude of
    relationship
    it is very confusing for people
    but
    positive relationship means that both
    variables are going in the same
    direction negative relationship means
    that the two variables are going in two
    different directions
    okay
    it's I don't know how to get it stuck in
    your head but positive just means same
    direction negative means different
    direction
    Pearson's R also tells you the magnitude
    meaning the strength of the relationship
    so direction of relationship as I said a
    positive r as X increases y increases
    so here's an example
    with more hours spent revising which is
    British English for studying your exam
    score goes up
    hours spent studying exam score
    goes up positive negative r as X
    increases y decreases
    X flu shots given
    positive like it goes up
    people with the flu goes down so this is
    a negative correlation because fluid
    shots are the things that are increasing
    and people with the flu is the thing
    that is decreasing
    okay
    magnitude of relationship this Pearson's
    R ranges from zero to one
    specifically negative one to positive
    one with zero in the middle the further
    from zero the stronger the relationship
    and the general rules obviously one's a
    perfect relationship
    0.7 to 0.9 correlation is a strong
    relationship and that's what we want 0.4
    to 0.6 is okay it's a moderate 0.1 to
    0.3 is weak it's nothing to get that
    excited about
    so let's look at an example of
    correlation
    this is from this website it's kind of a
    joke
    um that about things that are correlated
    it's really important to know that
    correlation can tell you that these two
    variables are related but correlation
    does not tell you that one causes the
    other
    you don't know is it possible that a
    third variable is the actual cause the
    relationship is spurious
    it also doesn't tell you the independent
    variable versus the dependent variable
    directionality in general we know that
    independent variables should come first
    in time before dependent variables but
    sometimes in life things happen at the
    same time
    so for example the divorce rate in Maine
    correlates with consumption of margarine
    which is like this fake butter and it's
    correlated pretty high the r is 0.99
    it's almost a perfect correlation and
    you can see this in the chart right
    um
    so obviously these two things have
    nothing to do with each other right and
    there's no independent or dependent
    variable but this is just to go to show
    you that really anything can be
    correlated
    here's number of people that drowned by
    falling into a pool correlates with
    films Nicholas Cage appeared in I mean
    this is ridiculous but it's correlated
    0.66 repeating
    so this website is pretty funny you can
    go to it if you search for Spurs
    correlations or I guess this guy's name
    is tigervegan.com Tyler vegan you can
    check it out but the really really
    important take home here is that
    correlation does not equal causation
    sing it from the rooftops tell people
    this is not case correlation just means
    that these variables are related that's
    it
    another part of correlation is r squared
    r squared is the coefficient of
    determination this is a correlation
    where you actually do have an
    independent and dependent variable
    unlike the examples that we just talked
    about
    r squared shows how much variation is in
    variable a the dependent variable that's
    explained by the variation in variable B
    the independent variable
    this is often used to indicate the
    strength of relationship again it
    doesn't indicate causality just the
    strength of the relationship but there
    is an independent and dependent variable
    so
    we use it to explain variation so in
    this example if age and earnings have a
    correlation of r as 0.60 then r squared
    is 0.36 which means that we can say that
    36 percent of the variation in the
    earnings variable is explained by
    variation in the variable age
    so that means that the rest of it is not
    explained by age and that means that
    there are other variables that
    contribute
    or in another example let's say you're
    doing a study of work-life balance and
    job satisfaction the r squared indicates
    the extent to which work-life balance
    can be explained by differences in job
    satisfaction a low r squared indicates
    that other factors might be influencing
    job satisfaction
    
    control variables we've talked about
    this a little bit before when we talked
    about experiments
    control variables determines if a
    relationship between A and B holds up if
    a third variable is introduced
    usually this is a variable that is held
    constant to isolate the effect of the
    independent variable on the dependent
    variable so for example imagine a study
    that looks at income as a dependent
    variable and how much education
    someone's had is the independent
    variable we would assume with the more
    education someone's had they probably
    make more money but age is an important
    control variable in the study because
    people that are older are also going to
    make more money right and it affects the
    impact that education has on income
    so age also influences income you have
    to hold it constant because otherwise it
    could confound the relationship between
    educational attainment and income
    the other form of Association that we
    talk about quite a bit is regression
    this is when there is more than one
    independent variable it is a multiple
    regression test
    this is how we explore multiple
    variables predicting a single dependent
    variable and they tell you how much each
    predictor variable in each independent
    variable entered into the model
    contributes to the dependent variable
    both one by one and also combined
    the simplest form of regression uses all
    continuous measures remember correlation
    that we just talked about was all
    continuous measures
    if you have a categorical variable that
    you want to put into regression you have
    to do this thing which is called
    creating a dummy variable
    where that categorical variable one
    value of it
    let's say it's like blue eyes versus
    brown eyes or something is given a one
    and the other values are given a zero
    um so you basically make it a continuous
    variable that is one and zero
    in a regression beta coefficients you
    can see this little character beta for
    each variable are the measures of the
    strength of the effect of entering each
    variable into the equation
    the higher the beta coefficient the
    stronger the predictor is and each beta
    has its own Associated significance
    level so here is a regression study I
    did many years ago that looked at
    internet use I looked at the age their
    household education or their household
    economic situation and their education
    this beta score age was
    um negatively related to using the
    internet and it was significant
    their economic situation was slightly
    significant or was still significant but
    a little bit less and education was
    pretty highly related so each of these
    was a contributor and then there was an
    overall measure as well that actually is
    that R square that we talked about
    a ton of independent
    Bluffton will enter these locks and
    as regression each block entered has its
    own r squared and the subsequent blocks
    build on it and what's really
    interesting is is that stepwise
    regression is actually really rigorous
    because that last independent variable
    still has to explain some of the
    variants even though all the previous
    variables already aided up in the model
    so there's videos textbook you can read
    more about this but now we're going to
    move on to talking about differences
    differences are testing the means
    between the two or three or four or
    whatever different groups or
    manipulations especially if you're doing
    an experiment
    the goal is to compute mean scores of
    the dependent variable for each category
    of the independent variable and compare
    them if you're doing an experiment
    you're definitely going to be doing a
    test of differences where you're
    comparing means
    but you could also take other data
    that's not experimental and do test of
    differences so for example do children
    divorce families do worse in schools
    than Children and non-diverse Families
    you get all the kids gpas
    you sort them by divorce family or not
    and you compare the average GPA this
    isn't an experiment
    you're using existing data I guess you
    have access to and doing a test between
    differences but again if you do an
    experiment you're definitely doing a
    test of differences
    so there's a couple different
    differences test
    the first one is called a Chi Squared
    even though it's spelled c-h-i you
    pronounce Chi like the word sky
    and that X there is the Greek letter Chi
    chi-square test allows you to look if
    there are actual data differences from a
    random distribution
    a chi-squared test requires you to use
    two categorical usually nominal
    variables
    you're going to see if there's
    significant associations between
    those variables we want to know if
    there's a difference in the distribution
    of responses or frequencies among these
    two groups
    in a chi-square you calculate the
    expected frequencies for each category
    assuming that there should be no
    difference significantly between the
    groups
    then you look at the observed
    frequencies and you compare the expected
    versus the
    actual
    so
    we tend to use a cross tab with this and
    let me show you an example
    so in our class data set I asked you if
    you played soccer on a team as a kid
    and also about people's eye color so
    in this case this is a cross tab you can
    see these are the class eye colors this
    is no I didn't play soccer on a team
    this is yes I played soccer on a team
    and this is how many of you in actuality
    The observed frequencies
    for each eye color category and each
    playing soccer or not category how many
    ended up in each cell
    but then I calculated this expected
    frequencies and how we calculate
    expected frequencies you can see for
    this example
    this is for didn't play soccer and brown
    I took the total number of people with
    brown eyes I multiplied it by the total
    number of people that did not play
    soccer and then I divided it by the
    total number of both and this gives me
    the expected frequencies
    so in this case it was expected that 20
    ish people would have brown eyes and say
    no
    Etc so if you look at the expected
    versus the The observed you can actually
    see that there were some unusual
    differences so for example up here hazel
    eyes
    in reality one person with these lies
    didn't play soccer
    five people with hazel eyes did but the
    expected were
    2.6 and 3.4 so you know just as an
    example
    and so what was pretty interesting this
    calculation is actually looking at the
    differences between the observed and
    expected
    with you know squared
    and you can see like I said this Hazel
    was
    a little different blue eyes was a
    little different than expected
    in this case
    the p-value was 0.29 and remember with
    our p-values we want them to be less
    than 0.05
    because that would mean that there was a
    greater than chance
    likelihood that the relationship
    actually was significant in this case it
    wasn't but also we wouldn't expect there
    to be right like there's no theoretical
    reason why playing soccer and eye color
    would have anything to do with each
    other I just had to use it for this
    example because it was the easiest
    categorical variables to use
    
    the other form of differences in the
    next way to think about this are is
    called t-test
    t-test allows you to compare two
    categories sample means against a
    continuous variable okay so this is a
    categorical variable that has two levels
    against a continuous variable
    there are independent and dependent
    t-tests
    an independent t-test
    is calculated
    the based on the size of the two groups
    and the amount of variation between the
    group two groups so these are groups
    that are not related to each other at
    all
    a dependent t-test the groups are
    closely related to each other like it
    would be like two parts of a partnered
    couple or a kid and their parent or
    something that you would expect the
    results to be really similar to each
    other
    um this is also the test that we use
    when we're testing the same people over
    time like if we were testing
    um their measures on some sort of IQ
    test at one point and then at some point
    in the future of okay but don't worry
    too much about this independent
    dependent remember that t-tests are two
    levels of a categorical variable
    looking against a continuous variable
    
    so for either independent or dependent
    test that grouping variable is the one
    that divides the sample into two groups
    it's usually going to be a nominal so
    for example imagine you had an
    experiment
    and you had a control condition or a
    treatment condition
    this is a variable that divides the
    sample into two groups people that were
    in the control are people that were in
    the treatment it's a nominal level
    measure
    because t-tests determine if means are
    different in the first group versus the
    second group the dependent variable has
    to be continuous interval or ratio level
    so that the means make sense
    
    so in this case I looked again at this
    soccer having played soccer yes or no so
    41 of you said that you played soccer as
    a kid 36 of you said you didn't
    and then I took this husky athletic
    support now this is not a true interval
    measure but we're treating it as such as
    you've read in the book that in
    communication we tend to treat it as
    such
    so
    I did a t-test and I said okay is there
    a difference between people that played
    soccer and didn't play soccer and what
    they said their support of UW husky
    Athletics is and in fact there was a
    significant relationship this p-value
    here is 0.00
    super interesting
    so if we look at this we can see
    that so green is people that played
    soccer and blue is people that didn't
    play soccer and you can actually see
    that people that played soccer were much
    more likely to be on the upper levels of
    support of husky Athletics and the
    people that didn't play soccer were much
    more likely to be on the lower level so
    this was interesting
    so soccer is a nominal variable remember
    this support is a fake interval and yeah
    we calculated this as a t-test so that
    was pretty interesting there is a
    significant relationship between having
    played soccer and
    husky athletic support in that there is
    a significant difference between those
    who played and didn't play
    the last test we're going to talk about
    is Anova Anova stands for analysis of
    variance Anova is the t-test older
    sibling Anova test differences between
    three or more categories sample means on
    a continuous variable so t-test has to
    be two levels in that case played soccer
    didn't
    and then on any continuous variable
    Anova three or more we're going to focus
    on one way and over there's a lot of
    different kinds of anovas but we'll use
    it one Way Anova for this example
    so in this example I looked at Ada eye
    color measure against this
    pseudo-interval measure of support for
    um
    husky Athletics so
    you can see here
    that when we went across eye colors and
    support that
    um there were some notable differences
    and you can look here and you can see
    like
    you know the the differences are a
    little bit different now what's really
    important is is that we had a ton more
    people in this class overall with brown
    eyes than any other color eyes so it's
    not really a fair uh comparison however
    if the p-value was significant
    um which is shocking
    um I think that the low numbers of some
    of these eye colors impacted the result
    but it did end up having a significant
    result
    um so there's no theoretical explanation
    for it but it did exist in here but so
    you can see that again that's soccer or
    I'm sorry the eye color is a nominal
    variable that had more than three
    categories we had in this case five and
    the continuous measure is a
    pseudo-continuous measure of support for
    Husky Athletics
    so again a t-test is two like soccer yes
    no Anova is three or more and this is
    different kinds of eye color
    

</h1>